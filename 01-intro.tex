\unnumberedchapter{Introduction}

\tdi{Intro, motivation, goals}

%\part{Theory and existing work}

\chapter{Cryptographic background}

\tdi{Intro}

\section{Cryptographic primitives}

The term \emph{cryptographic primitives} is used to refer to frequently used algorithms that serve as building blocks for more complex cryptographic protocols or systems.
They are designed to perform a single, very specific task.
This limitation of scope allows them to be simple, reliable and secure.

Each cryptographic primitive provides some (usually quite low-level) function and gives guarantees about it's security, provided some specified requirements are satisfied by the user. 
For example, a symmetric cipher might guarantee no attack faster than brute force exists, provided the same IV (\emph{initialization vector} -- see later) is never used twice.

There are many well known cryptographic primitives that have been in use for many years and have been the targets of many researchers for just as long.
Using them when designing any cryptographic applications makes this process less error prone, as all the designer has to do is keep the requirements satisfied instead of having to invent new primitives and then prove their security.

The most common primitives, many of which will be important for us in this work, are:
\begin{description}
\item[One-way hash functions] such as \emph{MD5}, \emph{SHA-1} or \emph{KECCAK}. \\
These produce a short \emph{digest} from arbitrarily long input message, however the reverse is hard to compute.
\item[Symmetric block and stream ciphers] such as \emph{AES} or \emph{Trivium}. \\
These encrypt and decrypt messages into \emph{ciphertext} and back using a \emph{secret key} which must be known for both steps.
\item[Asymmetric ciphers] such as \emph{RSA}. \\ 
These also provide encryption and decryption, however the two processes use two different keys, one of which can be publicly distributed.
\item[Digital signatures] such as \emph{DSA}, \emph{ElGamal} or \emph{Rabin}. \\
These can be used as a digital equivalent of signatures, proving the authorship and authenticity of a message.
\item[And many others] such as \emph{commitment schemes}, \emph{authentication} etc. which we will not be a focus of this work.
\end{description}

\section{Hash functions}
The most important cryptographic primitive for our purposes are \emph{hash functions}.
These are designed to reduce a long input \emph{message} into a fixed-length \emph{digest} (short from \emph{message digest}, also called \emph{hash}).
These functions should be easy to compute this way but infeasible\footnote{When we call a problem \emph{infeasible} we mean the computation should take exponential time, and thus is not practical for sufficient input sizes.} when going the other way -- from digest to message.

Formally, hash function $h$ is a function $h: X \to Y$ where $X$ is the (unbounded) set of inputs (messages) and $h(x) \in Y = \{0, 1\}^n$ is an n-bit long output digest.
The properties which we require this function to have are:

\begin{description}
\item[Pre-image resistance] \hfill \\
Given a hash $h$ it is hard to find a message $m$ such that $h(m) = h$.
\item[Second pre-image resistance] \hfill \\
Given a message $m_1$ it is hard to find a message $m_2$ such that $h(m_1) = h(m_2)$ and $m_1 \neq m_2$.
\item[Collision resistance] \hfill \\
It is hard to find messages $m_1, m_2$ such that $h(m_1) = h(m_2)$ and $m_1 \neq m_2$.
\end{description}

It is easy to see that when a hash function lacks one of these properties it also lacks all the following ones.
For example, if it's not second pre-image resistant it will also not be collision resistant.

Another useful and desirable property is the \emph{avalanche effect} \citep{katz2007introduction} -- changing a small part of the input message (for example, flipping a single bit) will have a large and unpredictable effect on the output digest (for example, half of the output bits flip).
For example, when we hash several close variations of the title of this thesis using \emph{SHA-1} we will get obviously different outputs:

\begin{tabular}{r l}
\texttt{Use of SAT Solvers in Cryptanalysis} & \texttt{2a39 79ea 37d6 4e4d 86eb} \\
& \texttt{fa77 3bb7 d027 3445 9e8f} \\
\texttt{Use of SAT-Solvers in Cryptanalysis} & \texttt{9cdb e658 e1a0 5454 ef0a}\\
& \texttt{0aae 7b07 f800 99b6 a2d2} \\
\texttt{Use of SAT-Solvers in cryptanalysis} & \texttt{83c4 3541 804d 3695 de67}\\
& \texttt{d9e5 064d 3db4 6113 4c22} \\
\texttt{Use of SAT Solvers in cryptanalysis} & \texttt{8753 1b8f fbf3 11dd ecf7} \\
& \texttt{7038 51ab a9d9 95f5 0fed}
\end{tabular}

Even though every two successive lines only differ in a single character (the dash in \emph{SAT-Solvers} and the capitalization of \emph{cryptanalysis}), the hashes are obviously different even on first glance.
This is useful for example when hash functions are used as \emph{checksums} for files published on the Internet.
The goal is to provide a short digest of a large file to verify that the downloaded file is identical to the authors' version -- the differences can arise from either malicious tampering or accidental network transmission errors.
After a user downloads this file they can compute the digest locally and verify that it matches the one provided.
Even if this verification is only done as a casual comparison by the user the avalanche effect will make the differences easy to spot.

\subsection{Merkle-Damg\aa rd construction}

Our definition of hash function requires that it will accept inputs of arbitrary length.
This is however harder to design than a function which operates on fixed size \emph{blocks}.
The Merkle-Damg\aa rd construction \citep{merkle1979secrecy,damgaard1990design} is a way to extend this fixed size function (called \emph{compression function}) into a proper hash function while preserving collision resistance.

Formally, given a compression function $f: \{0,1\}^{n+r+1} \to \{0,1\}^n$ with $n,r \ge 1$ we will create a hash function $h: \{0,1\}^* \to \{0,1\}^n$ like this: Take the input $x$ of length $l$ and split it into $t$ blocks of length $r$ each.
\tdi{padding}

Then we define $h(x) := H_{t+1}$ with (the $||$ operation represents concatenation of binary strings)
\begin{align*}
H_1 &= f(0^{n+1} ~||~ x_1) \\
H_i &= f_i(H_{i-1} ~||~ 1 ~||~ x_i)
\end{align*}

The following theorem shows that this construction preserves the collision resistance of the compression function $f$:

\begin{theorem}
If the compression function $f$ is collision resistant then also the hash function $h$ constructed as above is also collision resistant.
\end{theorem}
\begin{proof}
Suppose we can effectively find a collision in $h$, that is a pair $x \neq x'$ such that $h(x) = h(x')$.
Let $|x| = \ell$ and $|x'| = \ell'$.
Also let $t$ and $t'$ be the number of blocks after padding and splitting the inputs $x$ and $x'$ respectively.
\\

First let's consider the case when $t = t'$.
This means that $f(H_t ~||~ 1 ~||~ x_{t+1}) = f(H'_t ~||~ 1 ~||~ x'_{t+1})$. 
If the arguments are different we have found a collision in $f$ which is contradiction with the assumption that $f$ is collision resistant.
If they are equal that means $x_{t+1} = x'_{t+1}$ and $f(H_{t-1} ~||~ 1 ~||~ x_t) = f(H'_{t-1} ~||~ 1 ~||~ x'_t)$.
Again, if the arguments are different we have a collision in $f$ so we will only consider the case when they are equal.

Repeating this argument $t$ times we will get $f(0^{n+1} ~||~ x_1) = f(0^{n+1} ~||~ x'_1)$.
Again, only considering the case when the argument are equal we get that $\forall i \in \{1,\ldots,n\}: x_i = x'_i$ which implies $x = x'$ and this is a contradiction with the assumption the inputs are different.
\\

Now let's consider the case (without loss of generality) when $t < t'$.
Using the previous argument and repeating it $t$ times we will get the equality $f(0^{n+1} ~||~ x_1) = f(H_{t'-t} ~||~ 1 ~||~ x'_{t'-t+1})$.
The $n+1$-th bit of the inputs is $0$ on the left side and $1$ on the right side, which means we have found different inputs for $f$ that produce a collision, again a contradiction with the assumption that $f$ is collision resistant. 
\end{proof}

Many common and widely used hash functions are based on this construction, however the details of the construction can be modified.
These include \emph{SHA-1} and the \emph{SHA-2} family (consisting of \emph{SHA-256}, \emph{SHA-512} and others).
To provide an example of a real hash function but also to serve as a basis for further discussion and analysis we will now describe these hash functions in detail.

\subsubsection{SHA-1}

\emph{SHA-1}, short for \emph{Secure Hash Algorithm} is a hash function designed by the \emph{NSA} (\emph{National Security Agency}) and published in 1995.
It produces output digest of $160$ bits, processing the input in blocks of $512$ bits.
The first block is always the same, a constant \emph{initialization vector} (IV).
The output of the last round of compression function (when processing the last input block) is the final output digest.

\tdi{block processing/padding + IV + construction}

The compression function $f$ itself is defined as follows, where $H$ are the parts of previous round's output and $M$ are the parts of
the current input block, all $32$ bits in length:
\[
f(H_0 ~||~ \cdots ~||~ H_4, ~ M_0 ~||~ \cdots ~||~ M_{15}) = \dots
\]
\tdi{SHA-1 details}

\subsubsection{SHA-2}
\tdi{SHA2 family}

\subsection{Sponge construction}
Another approach to constructing hash functions is the \emph{sponge} construction, which is used for example by the \emph{SHA-3} family.
The idea is to use an internal hidden stat that will be initialized by the input message and then used to produce the output digest.

Let's have an input $x$ split into several blocks $x_1, \dots, x_t$, each of size $r$ bits.
\tdi{padding}

The sponge hash function consist of the following components:
\begin{description}
\item[State memory $S = R ~||~ C$] \hfill \\
A vector of $b = r+c$ bits, where the first $r$ bits $R$ are used to input the message and output the digest and the remaining $c$ bits $C$ are internal and never exposed directly.
\item[Sponge function $f$] \hfill \\
A function $f: \{0,1\}^r\times\{0,1\}^c \to \{0,1\}^r\times\{0,1\}^c$ that permutes or otherwise transforms the state memory.
\end{description}

The hashing process then consists of two phases -- \emph{absorbing} and \emph{squeezing}.
First, we initialize $R_0 := 0^r$ and $C_0 := 0^c$. Then we define
\[
R_i, C_i = f(R_{i-1} \oplus x_i,~ C_{i-1})
\]

After $t$ rounds the entire message will be \emph{absorbed} into the state $S_t = R_t ~||~ C_t$.

To produce an output digest $y$ composed of $t'$ blocks $y_1, \dots, y_{t'}$ of length $r$ bits we will continue similarly:
\[
R_i, C_i = f(R_{i-1},~ C_{i-1})
\]
and the output will \emph{squeezed} out of the state as $y_i = R_{t+i-1}$.
\tdi{Sponge security claims/proofs}

\subsubsection{SHA-3}
\tdi{sha3/keccak}

\subsection{Attacks on hash functions}

We will now describe several attacks on hash functions, starting with a simple one that demonstrates why we need a hash function that is hard to invert.

\subsubsection{Collision finding for invertible hash functions}

Let's assume that for a given hash function $h: X \to Y$ we can easily compute the inverse function $h^{-1}: Y \to X$.
We can then use the following algorithm to find a collision in $h$, thus showing that $h$ is not collision resistant.

\begin{algorithm}
\begin{algorithmic}[1]
	\While{$true$}
		\AlgLetRandom{$x$}{$X$}
		\AlgLet{$x'$}{$h^{-1}(h(x))$}
		\If{$x \neq x'$}
			\State \Return{$(x, x')$}
		\EndIf
	\EndWhile
\end{algorithmic}
\end{algorithm}

The notation $x \overset{\$}{\gets} X$ means that we assign a random element from $X$ into $x$.
If this algorithm finishes and returns a pair $x, x'$ this is guaranteed to be a collision.

What is the expected number of rounds this algorithm must execute in order to find a collision?
First, we will calculate the probability of finding of finding a collision in one iteration.

Let's define an equivalence relation $\sim$ over $X$ as $x \sim x' \Leftrightarrow h(x) = h(x')$. 
This relation forms equivalence classes of $X$ representing the sets of inputs which collide:
\[
[x] = \{x' \in X ~|~ h(x) = h(x')\}
\]

Let's call $C$ the set of all equivalence classes of $X$, that is
\[
X = \bigcup_{x \in C} [x]
\]

Since there are only $|Y|$ possible outputs this means that $|C|\le|Y|$. 

The probability of success for a given $x \in X$ is equal to $p_x = \frac{|[x]| - 1}{|[x]|}$, since the only way to fail is that the inverse function returns $x' = x$ but it must hold that $x' \in [x]$.
From this we can calculate the probability of a single round succeeding as:

\begin{align*}
p_r &= \frac{1}{|X|}\sum_{x\in X} p_x = \frac{1}{|X|}\sum_{x\in X} \frac{|[x]| - 1}{|[x]|} = \frac{1}{|X|}\sum_{c\in C}\sum_{x\in c}\frac{|c| - 1}{|c|} = \\
&= \frac{1}{|X|}\sum_{c\in C} |c| - 1 = \frac{1}{|X|}\left(\sum_{c\in C} |c| - \sum_{c\in C}1\right) = \frac{1}{|X|}\left(|X| - |C|\right)\\
&=1 - \frac{|C|}{|X|} \ge 1 - \frac{|Y|}{|X|}
\end{align*}

Assuming finite $X$ (we can restrict it in the case of hash function with unbounded input), and assuming $|X| \gg |Y|$ -- since otherwise the hash function doesn't reduce it's input and is therefore pointless -- this value will be close to $1$.

The algorithm will end after the first successful round therefore the number of rounds has a geometric distribution.
The expected number of rounds will be

\[
E = \frac{1}{p_r} \le \frac{1}{1-\frac{|Y|}{|X|}} = \frac{|X|}{|X|-|Y|}
\]

We have shown that the algorithm will terminate after a bounded finite rounds in the expected case and will find a collision in $h$.
It is therefore essential that the hash function is not easily invertible.

\subsubsection{Birthday attack}
Another attack that works for any hash function $h: X \to Y$ is the \emph{birthday attack}, named for the \emph{birthday paradox}.

The original statement of the birthday paradox asks how many people have to be in a room for the probability of any two sharing a birthday to reach $50\%$. 
It's easy to see that for a $100\%$ certainty we need $367$ people. However, for $50\%$ much less are required -- only $23$. Similarly, only about $70$ people are required for a $99\%$ probability.

This might seem counter-intuitive at first, hence the name paradox.
It follows from the fact that for $k$ people there are ${k \choose 2} \approx k^2$ possible pairs that can share a birthday.

This leads to an collision finding birthday attack that works as follows:

\begin{algorithm}
\begin{algorithmic}[1]
	\While{$true$}
		\AlgLetRandom{$x_1, \cdots, x_k$}{$X$} \Comment {All $x_i$ are chosen distinct.}
		\AlgLet{$y_i$}{$h(x_i)$}
		\If{$\exists i\neq j: y_i = y_j$}
			\State \Return{$(x_i, x_j)$}
		\EndIf
	\EndWhile
\end{algorithmic}
\end{algorithm}

Each round takes $\bigO{k}$ time, where $k$ is a parameter to be chosen later.

The probability of success for a single round can be show to be
\[
p_r \ge 1 - e^\frac{-k(k-1)}{2|Y|}
\]
\tdi{add proof / cite some source}

If we can make $p_r \ge \epsilon$ where $\epsilon$ is some constant, the expected number of rounds will be constant (since it follows a geometric distribution, see previous section).

It can be show that for $\epsilon = \frac{1}{2}$ we have $k = \bigO{\sqrt{|Y|}}$ and the expected number of rounds will be $2$.

Therefore for a hash function with output size of $n$ bits the birthday attack can be expected to find a collision in time $\bigO{2^\frac{n}{2}}$.

\tdi{Other hash function attacks / attack types}

\tdi{Ciphers - AES}
\tdi{Cipher attacks}

\chapter{SAT Solving}

In this work we will be using \emph{SAT solvers} to find solutions to a complex \emph{SAT instance} describing some cryptographic primitive.
In order to do this efficiently we need to understand how they work and how to utilize them properly.

\emph{SAT solvers} are programs which search for a \emph{model} (a satisfying assignment) for an input logic formula.
This is usually provided as a list of clauses in \emph{conjunctive normal form} (CNF) -- each clause contains a disjunction of literals and all clauses must be true for the formula to be satisfied.
Since this is a \emph{normal form}, meaning that every formula can be written this way, it is not limiting in any way but is simpler to work with.

Since \emph{SAT} is an $NP$-complete language and assuming $P \neq NP$ we can't in general do better than an exponential time algorithm, which tries every possible boolean truth assignment.
To remember which assignments have been tried and which haven't we can implement this algorithm as a recursive backtracking, where for every input variable we try in turn both possible values.
This will only require linear memory to implement.

\section{Improving the naive algorithm}

While we can't get a asymptotically faster algorithm for the worst case we can still improve on it significantly by avoiding paths which are guaranteed to lead to a \emph{conflict} -- an partial assignment that can not be extended into any satisfying one.

\subsection{DPLL algorithm}

First such improvement is the \emph{DPLL algorithm} (Davis-Putnam-Logemann-Loveland, \citep{davis1960computing,davis1962machine}) which improves upon the naive backtracking algorithm by applying two rules at each step:

\begin{description}
\item[Unit propagation] \hfill \\
We will call a clause a \emph{unit} if it only contains one unassigned literal.
If the clause is already satisfied we can ignore it during further search.
Otherwise it can only be satisfied by assigning the necessary value to the remaining literal.
This means there will be no branching for this literal and thus the search time will be reduced by avoiding the obviously wrong choice.

\item[Pure literal elimination] \hfill \\
We will call a variable \emph{pure} if it only occurs in the formula with a single polarity (that is, always negated or always non-negated).
All pure literals can be assigned the required value to make all clauses containing them true and these clauses can then also be ignored during further search.
\end{description}

These two simple rules will help to reduce the search space the algorithm has to look through and thus reduce the total running time.

\subsection{Conflict-Driven Clause Learning}

It can happen that after some partial assignment of variables there is already a conflict and it is not possible to extend this assignment to a satisfying one.
However, the DPLL algorithm will continue to explore the entire search subspace.
With \emph{conflict-driven clause learning} (CDCL, \citep{bayardo1997using,marques1999grasp}) we can avoid this by the use of \emph{implication graph} and \emph{learnt clauses}.

The \emph{implication graph} is a directed graph in which vertices are variables with their assignment.
These will be marked as either \emph{decision} or \emph{forced}.

At first we start with an empty implication graph and execute the DPLL algorithm.
Every time we make an arbitrary decision (the recursive step in the algorithm) we will store add a decision vertex representing this variable and the assignment we have chosen into the graph.
Every time either of the two rules of the DPLL algorithm forces some assignment of a variable we will add a forced vertex into the graph, with edges from every vertex that is a part of this forced choice.

For example, with clause $(x \lor y \lor z)$ and with partial assignment $x=0, y=0$ the unit propagation rule will force the assignment of $z=1$.
We will add this as a forced vertex, with edges from both $x$ and $y$.

When we add an vertex to the graph for some variable but this variable is already present with the opposite assignment we have found a conflict.
Now comes the part of CDCL which speeds up this search -- we will find all the decision vertices (variables) that are responsible for this conflict.
Let us call them $x_1, \dots, x_k$ with assignments $v_1, \dots, v_k$.
From this we can build a \emph{learnt clause} $L$:
\begin{align*}
(x_1 = v_1 \land \dots \land x_k = v_k) &\Rightarrow \bot \\
\overline{\bot} &\Rightarrow \overline{(x_1 = v_1 \land \dots \land x_k = v_k)} \\
\top &\Rightarrow (x_1 = \overline{v_1} \lor \dots \lor x_k = \overline{v_k}) \\
L &:= (x_1^{(\overline{v_1})} \lor \dots \lor x_k^{(\overline{v_k})})
\end{align*}

The notation $x^{(v)}$ is used to represent $x$ if $v=1$ and $\overline{x}$ if $v=0$. Symbols $\top$ and $\bot$ represent tautology and contradiction, respectively.

We can now add this learnt clause $L$ into the list of clauses we have to satisfy, since not satisfying it is guaranteed to lead to a conflict.
In the next step of the backtracking algorithm we will not return only a single level up from the recursion (that is, removing the assigning of just a single variable) but all the way back until we reach the first point where one of the conflicting variables was assigned a value.

This method is used in all modern SAT solvers and improves their performance significantly over the naive algorithm.
\tdi{solver heuristics}

\tdi{extending}
\tdi{overview of solvers}
\tdi{benchmarks, ...}

\chapter{Attacks}

\tdi{existing overview}
\tdi{details of few}
\tdi{Plans - partial preimage, ...}

\chapter{Encoding}

\tdi{existing tools - pretty much none}
\tdi{sha1 code/approach overview}
\tdi{goals}
\tdi{methodology}
\tdi{implementation}

\chapter{Results}

\tdi{measurements}
\tdi{analysis}

\unnumberedchapter{Conclusions and future work}
\tdi{...}